{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 : First order methods on regression models\n",
    "\n",
    "#### Authors: S. Gaiffas, A. Gramfort\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- proximal gradient descent (ISTA)\n",
    "- accelerated gradient descent (FISTA) \n",
    "\n",
    "for \n",
    "- linear regression\n",
    "- logistic regression \n",
    "\n",
    "models.\n",
    "\n",
    "The proximal operators we will use are the \n",
    "- ridge penalization\n",
    "- L1 penalization\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 9th of october at 23:59**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called **Rendu TP du 3 octobre 2016**. This is where you submit your jupyter notebook file. \n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"camille\"\n",
    "ln1 = \"masset\"\n",
    "fn2 = \"boris\"\n",
    "ln2 = \"muzellec\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp1\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## to embed figures in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 : Introduction\n",
    "\n",
    "We'll start by generating sparse vectors and simulating data\n",
    "\n",
    "### Getting sparse coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)  # to have simpler print outputs with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "n_samples = 1000\n",
    "idx = np.arange(n_features)\n",
    "coefs = (-1) ** (idx - 1) * np.exp(-idx / 10.)\n",
    "coefs[20:] = 0.\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Parameters / Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the simulation of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu_linreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a linear regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape=(n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    # Construction of a covariance matrix\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    # Simulation of features\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    # Simulation of the labels\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    return A, b\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function (overflow-proof)\"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def simu_logreg(coefs, n_samples=1000, corr=0.5):\n",
    "    \"\"\"Simulation of a logistic regression model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coefs : `numpy.array`, shape=(n_features,)\n",
    "        Coefficients of the model\n",
    "    \n",
    "    n_samples : `int`, default=1000\n",
    "        Number of samples to simulate\n",
    "    \n",
    "    corr : `float`, default=0.5\n",
    "        Correlation of the features\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : `numpy.ndarray`, shape=(n_samples, n_features)\n",
    "        Simulated features matrix. It samples of a centered Gaussian \n",
    "        vector with covariance given by the Toeplitz matrix\n",
    "    \n",
    "    b : `numpy.array`, shape=(n_samples,)\n",
    "        Simulated labels\n",
    "    \"\"\"\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p = sigmoid(A.dot(coefs))\n",
    "    b = np.random.binomial(1, p, size=n_samples)\n",
    "    b[:] = 2 * b - 1\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Samples data to test functions\n",
    "coefs = randn(n_features)\n",
    "A, b = simu_linreg(coefs, n_samples=1000, corr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Proximal operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the proximal operator of a fonction $g$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{prox}_g(y, t) = \\arg\\min_x \\Big\\{ \\frac 12 \\|x - y\\|_2^2 + t g(x) \\Big\\}.\n",
    "$$\n",
    "\n",
    "where $t \\geq 0$ is a non-negative number.\n",
    "We have in mind to use the following cases\n",
    "\n",
    "- Ridge penalization, where $g(x) = \\frac{s}{2} \\|x\\|_2^2$\n",
    "- Lasso penalization, where $g(x) = s \\|x\\|_1$\n",
    "\n",
    "where $s \\geq 0$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Code a function that computes $g(x)$ in both cases and $\\text{prox}_g(x)$ for ridge and  lasso penalization (use the slides of the first course to get the formulas), using the prototypes given below\n",
    "- Visualize the functions applied element wise by the proximity operators of the Ridge and Lasso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prox_lasso(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the Lasso at x with strength t\"\"\"\n",
    "    return np.multiply(np.sign(x), np.maximum(np.absolute(x) - s*t, 0))\n",
    "    \n",
    "def lasso(x, s):\n",
    "    \"\"\"Value of the Lasso penalization at x with strength t\"\"\"\n",
    "    return s*np.linalg.norm(x, 1)\n",
    "\n",
    "def prox_ridge(x, s, t=1.):\n",
    "    \"\"\"Proximal operator for the ridge at x with strength t\"\"\"    \n",
    "    return 1 / (1 + s*t) * x\n",
    "    \n",
    "def ridge(x, s):\n",
    "    \"\"\"Value of the ridge penalization at x with strength t\"\"\"\n",
    "    return s / 2 * np.linalg.norm(x, 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We are now going to visualize the effect of the proximity operators on coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = randn(50)\n",
    "l_l1 = 1.\n",
    "l_l2 = 0.5\n",
    "\n",
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(x)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(prox_lasso(x, s=l_l1))\n",
    "plt.title(\"Proximal Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(prox_ridge(x, s=l_l2))\n",
    "plt.title(\"Proximal Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "- Comment what you observe (1 or 2 sentences).\n",
    "\n",
    "Proximal Lasso induces sparsity: all coefficients with absolute value under $s\\cdot t$ vanish, and the others have their absolute value reduced by $s\\cdot t$.\n",
    "\n",
    "Proximal Ridge has a less violent action: it scales down coefficients (by the same factor $\\frac{1}{1+s\\cdot t}$), but no coefficient vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradients\n",
    "\n",
    "The problems we want to minimize take the form:\n",
    "$$\n",
    "\\arg\\min_x f(x) + g(x)\n",
    "$$\n",
    "where $f$ is $L$-smooth and $g$ is prox-capable.\n",
    "\n",
    "We will consider below the following cases\n",
    "\n",
    "**Linear regression**, where \n",
    "$$\n",
    "f(x) = \\frac{1}{2n} \\sum_{i=1}^n (b_i - a_i^\\top x)^2 = \\frac{1}{2 n} \\| b - A x \\|_2^2,\n",
    "$$\n",
    "where $n$ is the sample size, $b = [b_1 \\cdots b_n]$ is the vector of labels and $A$ is the matrix of features.\n",
    "\n",
    "**Logistic regression**, where\n",
    "$$\n",
    "f(x) = \\frac{1}{n} \\sum_{i=1}^n \\log(1 + \\exp(-b_i a_i^\\top x)),\n",
    "$$\n",
    "where $n$ is the sample size, and where labels $b_i \\in \\{ -1, 1 \\}$ for all $i$.\n",
    "\n",
    "We need to be able to compute $f$ and its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- Compute on paper the gradient $\\nabla f$ of $f$ for both cases (linear and logistic regression)\n",
    "- Code a function that computes $f$ and its gradient $\\nabla f$ in both cases, using the prototypes below.\n",
    "- Check that these functions are correct by numerically checking the gradient, using the function ``checkgrad`` from ``scipy.optimize``. Remark: use the functions `simu_linreg` and `simu_logreg` to simulate data according to the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function for the _linear regression_ model is:\n",
    "$\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$.\n",
    "\n",
    "The gradient of the cost function for the _logistic regression_ model is:\n",
    "$\\nabla f(x) = -\\frac{1}{n}(b\\mathbf{1}^\\top \\odot A) \\odot \\sigma(-b \\odot Ax)$ where $\\odot$ is the element-wise product, $\\sigma$ is the sigmoid function, and $\\mathbf{1} = (1, \\dots, 1)^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_linreg(x):\n",
    "    \"\"\"Least-squares loss\"\"\"\n",
    "    return np.linalg.norm(b - A.dot(x))**2 / (2*A.shape[0])\n",
    "\n",
    "def grad_linreg(x):\n",
    "    \"\"\"Leas-squares gradient\"\"\"\n",
    "    return A.T.dot(A.dot(x) - b) / A.shape[0]\n",
    "\n",
    "def loss_logreg(x):\n",
    "    \"\"\"Logistic loss\"\"\"\n",
    "    return np.log(1 + np.exp(-np.multiply(b, A.dot(x)))).sum() / A.shape[0]\n",
    "\n",
    "def grad_logreg(x):\n",
    "    \"\"\"Logistic gradient\"\"\"\n",
    "    return -np.multiply(np.multiply(b.reshape((-1, 1)), A), sigmoid(-np.multiply(b, A.dot(x))).reshape((-1, 1))).sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "print(check_grad(loss_linreg, grad_linreg, randn(50)))\n",
    "print(check_grad(loss_logreg, grad_logreg, randn(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a function to compute $f$, $\\nabla f$ and $g$ and $\\text{prox}_g$. \n",
    "\n",
    "We want now to code the Ista and Fista solvers to minimize\n",
    "\n",
    "$$\n",
    "\\arg\\min_x f(x) + g(x)\n",
    "$$\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "- Implement functions that compute the Lipschitz constants for linear and \n",
    "  logistic regression losses. Note that the operator norm of a matrix can \n",
    "  be computed using the function `numpy.linalg.norm` (read the documentation\n",
    "  of the function)\n",
    "\n",
    "- Finish the functions `ista` and `fista` below that implements the \n",
    "  ISTA (Proximal Gradient Descent) and FISTA (Accelerated Proximal \n",
    "  Gradient Descent) algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lip_linreg(A):\n",
    "    \"\"\"Lipschitz constant for linear squares loss\"\"\"    \n",
    "    return np.linalg.norm(A.T.dot(A), 2) / A.shape[0]\n",
    "    \n",
    "def lip_logreg(A):\n",
    "    \"\"\"Lipschitz constant for logistic loss\"\"\"    \n",
    "    return lip_linreg(A) / 4\n",
    "    \n",
    "def ista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    from scipy.optimize import check_grad\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "    \n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    if verbose:\n",
    "        print (\"Lauching ISTA solver...\")\n",
    "        print (' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "\n",
    "        x = prox_g(x - step*grad_f(x))\n",
    "        \n",
    "        obj = f(x) + g(x, s)\n",
    "        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print (' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, objectives, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fista(x0, f, grad_f, g, prox_g, step, s=0., n_iter=50,\n",
    "         x_true=coefs, verbose=True):\n",
    "    \"\"\"Accelerated Proximal gradient descent algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    x_new = x0.copy()\n",
    "    # An extra variable is required for FISTA\n",
    "    z = x0.copy()\n",
    "    n_samples, n_features = A.shape\n",
    "    # estimation error history\n",
    "    errors = []\n",
    "    # objective history\n",
    "    objectives = []\n",
    "    # Current estimation error\n",
    "    err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "    errors.append(err)\n",
    "    # Current objective\n",
    "    obj = f(x) + g(x, s)\n",
    "    objectives.append(obj)\n",
    "    t = 1.\n",
    "    t_new = 1.    \n",
    "    if verbose:\n",
    "        print (\"Lauching FISTA solver...\")\n",
    "        print (' | '.join([name.center(8) for name in [\"it\", \"obj\", \"err\"]]))\n",
    "    for k in range(n_iter + 1):\n",
    "\n",
    "        x_new = prox_g(z - step*grad_f(z))\n",
    "        t_new = (1 + np.sqrt(1+4*t**2)) / 2\n",
    "        z_new = x_new + (t-1) / t_new * (x_new - x)\n",
    "        x, z, t = x_new, z_new, t_new\n",
    "        \n",
    "        obj = f(x) + g(x, s)\n",
    "        err = np.linalg.norm(x - x_true) / np.linalg.norm(x_true)\n",
    "        errors.append(err)\n",
    "        objectives.append(obj)\n",
    "        if k % 10 == 0 and verbose:\n",
    "            print (' | '.join([(\"%d\" % k).rjust(8), \n",
    "                              (\"%.2e\" % obj).rjust(8), \n",
    "                              (\"%.2e\" % err).rjust(8)]))\n",
    "    return x, np.array(objectives), np.array(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms comparison and numerical experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some definitions before launching the algorithms\n",
    "x0 = np.zeros(n_features)\n",
    "n_iter = 100\n",
    "s = 1e-2\n",
    "coefs = randn(n_features)\n",
    "A, b = simu_linreg(coefs, n_samples=1000, corr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- Compute a precise minimum and a precise minimizer of the linear regression with ridge \n",
    "  penalization problem using the parameters give above. This can be done by using fista with \n",
    "  1000 iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run fista over 1000 iterations to estimate the true minimizer\n",
    "L = lip_linreg(A)\n",
    "\n",
    "x_min, obj_min, err_min = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x, s, 1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the convergences of ISTA and FISTA, in terms of distance to the minimum and \n",
    "  distance to the minimizer. Do your plots using a logarithmic scale of the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "x1, obj1, err1 = ista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x, s, 1/L), 1/L, s, n_iter,\n",
    "         x_true=x_min, verbose=False)\n",
    "\n",
    "x2, obj2, err2 = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x, s, 1/L), 1/L, s, n_iter,\n",
    "         x_true=x_min, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1)), err1, label=\"ISTA error\")\n",
    "ax.semilogy(np.arange(len(err2)), err2, label=\"FISTA error\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(obj1)), obj1 - np.amin(obj_min), label=\"ISTA objective\")\n",
    "ax.semilogy(np.arange(len(obj2)), obj2 - np.amin(obj_min), label=\"FISTA objective\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "s = 1e-3\n",
    "A, b = simu_logreg(coefs, n_samples=1000, corr=0.5)\n",
    "L = lip_logreg(A)\n",
    "\n",
    "# Again, estimate the true minimizer using 1000 iterations of FISTA\n",
    "\n",
    "x_min, obj_min, err_min = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "\n",
    "# Now run the algorithms\n",
    "\n",
    "x1, obj1, err1 = ista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_min, verbose=False)\n",
    "\n",
    "x2, obj2, err2 = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_min, verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1)), err1, label=\"ISTA error\")\n",
    "ax.semilogy(np.arange(len(err2)), err2, label=\"FISTA error\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(obj1)), obj1 - np.amin(obj_min), label=\"ISTA objective\")\n",
    "ax.semilogy(np.arange(len(obj2)), obj2 - np.amin(obj_min), label=\"FISTA objective\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the correlation \n",
    "  of the features on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, study the influence of the level of ridge \n",
    "  penalization on the performance of the optimization algorithms. Explain.\n",
    "\n",
    "- In linear regression and logistic regression, compare the performance of the optimization\n",
    "  algorithms for ridge and lasso penalizations. Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Réponses**\n",
    "\n",
    "- plotter les coeffs \"true\" et inférés, en fonction de la corrélation\n",
    "- plotter le rang de l'itération à laquelle l'erreur passe sous un seuil fixé, en fonction de la corrélation (faire une centaine de tirage par valeur de corrélation, et plotter la moyenne/variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# An example with high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample problem parameters\n",
    "coefs = randn(n_features)\n",
    "A, b = simu_logreg(coefs, n_samples=1000, corr=0.9)\n",
    "\n",
    "# Logistic regression\n",
    "s = 1e-2\n",
    "L = lip_logreg(A)\n",
    "\n",
    "\n",
    "# Again, estimate the true minimizer using 1000 iterations of FISTA\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x_minr, obj_minr, err_minr = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x_minl, obj_minl, err_minl = fista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x1r, obj1r, err1r = ista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "x2r, obj2r, err2r = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x1l, obj1l, err1l = ista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n",
    "\n",
    "x2l, obj2l, err2l = fista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1r)), err1r, label=\"ISTA error - ridge\")\n",
    "ax.semilogy(np.arange(len(err2r)), err2r, label=\"FISTA error - ridge\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1l)), err1l, label=\"ISTA error - lasso\")\n",
    "ax.semilogy(np.arange(len(err2l)), err2l, label=\"FISTA error - lasso\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(x2l)\n",
    "plt.title(\"Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(x2r)\n",
    "plt.title(\"Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample problem parameters\n",
    "A, b = simu_linreg(coefs, n_samples=1000, corr=0.9)\n",
    "\n",
    "# Linear regression\n",
    "s = 1e-2\n",
    "L = lip_linreg(A)\n",
    "\n",
    "\n",
    "# Again, estimate the true minimizer using 1000 iterations of FISTA\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x_minr, obj_minr, err_minr = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x_minl, obj_minl, err_minl = fista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x1r, obj1r, err1r = ista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "x2r, obj2r, err2r = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x1l, obj1l, err1l = ista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n",
    "\n",
    "x2l, obj2l, err2l = fista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1r)), err1r, label=\"ISTA error - ridge\")\n",
    "ax.semilogy(np.arange(len(err2r)), err2r, label=\"FISTA error - ridge\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1l)), err1l, label=\"ISTA error - lasso\")\n",
    "ax.semilogy(np.arange(len(err2l)), err2l, label=\"FISTA error - lasso\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(x2l)\n",
    "plt.title(\"Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(x2r)\n",
    "plt.title(\"Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example with weak correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample problem parameters\n",
    "coefs = randn(n_features)\n",
    "A, b = simu_logreg(coefs, n_samples=1000, corr=0.1)\n",
    "\n",
    "# Logistic regression\n",
    "s = 1e-2\n",
    "L = lip_logreg(A)\n",
    "\n",
    "\n",
    "# Again, estimate the true minimizer using 1000 iterations of FISTA\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x_minr, obj_minr, err_minr = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x_minl, obj_minl, err_minl = fista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x1r, obj1r, err1r = ista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "x2r, obj2r, err2r = fista(x0, loss_logreg, grad_logreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x1l, obj1l, err1l = ista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n",
    "\n",
    "x2l, obj2l, err2l = fista(x0, loss_logreg, grad_logreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1r)), err1r, label=\"ISTA error - ridge\")\n",
    "ax.semilogy(np.arange(len(err2r)), err2r, label=\"FISTA error - ridge\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1l)), err1l, label=\"ISTA error - lasso\")\n",
    "ax.semilogy(np.arange(len(err2l)), err2l, label=\"FISTA error - lasso\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(x2l)\n",
    "plt.title(\"Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(x2r)\n",
    "plt.title(\"Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample problem parameters\n",
    "A, b = simu_linreg(coefs, n_samples=1000, corr=0.1)\n",
    "\n",
    "# Linear regression\n",
    "s = 1e-2\n",
    "L = lip_linreg(A)\n",
    "\n",
    "\n",
    "# Again, estimate the true minimizer using 1000 iterations of FISTA\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x_minr, obj_minr, err_minr = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x_minl, obj_minl, err_minl = fista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, 1000,\n",
    "         x_true=coefs, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Ridge\n",
    "\n",
    "x1r, obj1r, err1r = ista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "x2r, obj2r, err2r = fista(x0, loss_linreg, grad_linreg, ridge, lambda x : prox_ridge(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minr, verbose=False)\n",
    "\n",
    "# Lasso\n",
    "\n",
    "x1l, obj1l, err1l = ista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n",
    "\n",
    "x2l, obj2l, err2l = fista(x0, loss_linreg, grad_linreg, lasso, lambda x : prox_lasso(x,s,1/L), 1/L, s, n_iter,\n",
    "         x_true=x_minl, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1r)), err1r, label=\"ISTA error - ridge\")\n",
    "ax.semilogy(np.arange(len(err2r)), err2r, label=\"FISTA error - ridge\")\n",
    "legend = ax.legend(loc='upper center')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(np.arange(len(err1l)), err1l, label=\"ISTA error - lasso\")\n",
    "ax.semilogy(np.arange(len(err2l)), err2l, label=\"FISTA error - lasso\")\n",
    "legend = ax.legend(loc='upper center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15.0, 4.0))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.stem(coefs)\n",
    "plt.title(\"Original parameter\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.stem(x2l)\n",
    "plt.title(\"Lasso\", fontsize=16)\n",
    "plt.ylim([-2, 2])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.stem(x2r)\n",
    "plt.title(\"Ridge\", fontsize=16)\n",
    "plt.ylim([-2, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
